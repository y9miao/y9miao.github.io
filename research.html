
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yang Miao</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- New GA4 tracking code, see https://support.google.com/analytics/answer/10271001#analyticsjs-enable-basic --> 
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GNJD50R0Z7"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-GNJD50R0Z7');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row div a {
        font-size: 14px; /* Adjust size as needed */
        font-weight: bold; /* Makes text bold */
        /* Add other styles as needed */
      }
    </style>

    <style>
      .paper-item {
        display: flex;
        align-items: flex-start;
        margin-bottom: 20px; /* Adjust the margin as needed */
      }
      
      .paper-img {
          width: 200px; /* Adjust width as needed */
          height: auto;
          margin-right: 15px; /* Adjust the margin between image and text */
      }
      
      .paper-content {
          flex-grow: 1;
      }
      
      .paper-content h3 {
          margin-top: 0;
      }
      
      .paper-content p {
          margin: 5px 0; /* Adjust for spacing between lines */
      }
      
      .paper-content a {
          margin-right: 10px; /* Space between links */
          text-decoration: none; /* Optional: removes underline from links */
          /* Add more styles for your links here */
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Yang Miao</h1>
        <p>PhD student in Computer Vision and Robotics at INSAIT</p>
        <h3><a href="https://y9miao.github.io">Home</a></h3>
            <h3><a href="https://y9miao.github.io/research.html">Publications</a></h3>
            <h3><a href="https://y9miao.github.io/projects.html">Selected Projects</a></h3>
        <h3><a href="https://y9miao.github.io/research/CV.pdf">CV</a></h3>  
        <p>
        <b>Links</b>
            <div><a href="https://scholar.google.com/citations?user=7TCTn7wAAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar-square"></i> Scholar</a></div>
            <div><a href="http://github.com/y9miao"><i class="fa fa-github-square"></i> GitHub</a></div>
            <div><a href="https://www.linkedin.com/in/yang-miao-415b39202/" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a></div>
        </p>
        
        <p>
        <b>Contact</b>
        <div class="social-row">
          <a href="yangmiaogz@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a>
        </div>
        </p>
      </header>

      <section>
        
        <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true">
            <span class="octicon octicon-link"></span></a>Papers under Review</h2>

        <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true">
          <span class="octicon octicon-link"></span></a>Published Papers</h2>
          <div class="paper-item">
            <img src="research/Articulate3D.png" alt="Paper Figure" class="paper-img">
            <div class="paper-content">
                <h3 style="font-size: 16px;">Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description</h3>
                <p style="margin:0"><button class="accordion">
                  Abstract
                </button>
                <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
                  We propose Articulate3D, an expertly curated dataset in the Universal Scene Description (USD) format, featuring high-quality manual annotations, for instance, segmentation and articulation on indoor scenes. 
                  Also, we come up with USDNet, a learning-based model together with a novel baseline capable of predicting part segmentation along with a full specification of motion attributes, including motion type, articulated and interactable parts, and motion parameters.
                </div>
                <!-- <p>Status: Under Review</p> -->
                <p>Authors: Anna-Maria Halacheva*, <b>Yang Miao*</b>, Jan-Nico Zaech, Xi Wang, Luc Van Gool, Danda Pani Paudel (* equal contribution)</p>
                <p style="font-size: 16px;"><i><b>ICCV 2025</b> </i></p>
                <a href="https://arxiv.org/abs/2412.01398">[preprint]</a>
                <a href="https://github.com/insait-institute/USDNet.git">[code]</a>
                <a href="https://insait-institute.github.io/articulate3d.github.io/">[webpage]</a>  
                <a href="https://insait-institute.github.io/articulate3d.github.io/challenge.html">[challenge]</a>
            </div>
          </div>

          <div class="paper-item">
            <img src="research/SGLocTeaser.jpg" alt="Paper Figure" class="paper-img">
            <div class="paper-content">
                <h3 style="font-size: 16px;">SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs</h3>
                <p style="margin:0"><button class="accordion">
                  Abstract
                </button>
                <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
                  We introduce a novel problem, i.e., the localization of an input image within a multi-modal reference map represented by a database of 3D scene graphs. 
                  These graphs comprise multiple modalities, including object-level point clouds, images, attributes, 
                    and relationships between objects, offering a lightweight and efficient alternative to conventional methods 
                    that rely on extensive image databases. 
                  Given the available modalities, the proposed method SceneGraphLoc learns a fixed-sized embedding for each node 
                    (i.e., representing an object instance) in the scene graph, enabling effective matching with the objects visible in the input query image. 
                  This strategy significantly outperforms other cross-modal methods, even without incorporating images into the map embeddings. 
                  When images are leveraged, SceneGraphLoc achieves performance close to that of state-of-the-art techniques depending on large image databases, 
                    while requiring three orders-of-magnitude less storage and operating orders-of-magnitude faster. The code will be made public.            
                </div>
                <!-- <p>Status: Under Review</p> -->
                <p>Authors: <b>Yang Miao</b>, Francis Engelmann, Olga Vysotska, Federico Tombari, Marc Pollefeys, Daniel Barath</p>
                <p style="font-size: 16px;"><i><b>ECCV 2024</b> </i></p>
                <a href="https://arxiv.org/pdf/2404.00469">[preprint]</a>
                <a href="https://youtu.be/_7YPGsMrVcQ">[video]</a>
                <a href="https://scenegraphloc.github.io/">[webpage]</a>  
            </div>
          </div>
  
          <div class="paper-item">
            <img src="research/PanoMappingTeaser.png" alt="Paper Figure" class="paper-img">
            <div class="paper-content">
                <h3 style="font-size: 16px;">Volumetric Semantically Consistent 3D Panoptic Mapping</h3>
                <p style="margin:0"><button class="accordion">
                  Abstract
                </button>
                <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
                  We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at generating comprehensive, accurate, 
                    and efficient semantic 3D maps suitable for autonomous agents in unstructured environments. 
                  The proposed approach is based on a Voxel-TSDF representation used in recent algorithms. 
                  It introduces novel ways of integrating semantic prediction confidence during mapping, producing semantic 
                    and instance-consistent 3D regions. 
                  Further improvements are achieved by graph optimization-based semantic labeling and instance refinement. 
                  The proposed method achieves accuracy superior to the state of the art on public large-scale datasets, 
                    improving on a number of widely used metrics. We also highlight a downfall in the evaluation of recent studies: 
                    using the ground truth trajectory as input instead of a SLAM-estimated one substantially affects the accuracy, 
                    creating a large gap between the reported results and the actual performance on real-world data.           
                </div>
                <!-- <p>Status: Under Review</p> -->
                <p>Authors: <b>Yang Miao</b>, Iro Armeni, Marc Pollefeys, Daniel Barath</p>
                <p style="font-size: 16px;"><i><b>IROS 2024</b> </i>(<b><span style="color: red;">Oral Pre</span></b>)</p>
                <a href="https://arxiv.org/abs/2309.14737">[preprint]</a>
                <a href="https://youtu.be/_7YPGsMrVcQ">[video]</a>
                <a href="https://github.com/y9miao/ConsistentPanopticSLAM">[code]</a>  
            </div>
          </div>

          <div class="paper-item">
            <img src="research/BulkcargoTeaser.png" alt="Paper Figure" class="paper-img">
            <div class="paper-content">
                <h3 style="font-size: 16px;">3D Computer Vision for Automation of Port Operations</h3>
                <p style="margin:0"><button class="accordion">
                  Abstract
                </button>
                <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
                  Achieving port automation of machinery at bulk terminals is a challenging problem due to the volatile operation environments and 
                    complexity of bulk loading compared to the situations in container terminals. 
                  In order to facilitate port automation, we present a method of hull modeling (reconstruction of hull’s structure) and operation target
                    (cargo holds under loading) identification based on 3D point cloud collected by Laser Measurement System mounted on the ship loader. 
                  In the hull modeling algorithm, we incrementally register pairs of point clouds and reconstruct the 3D structure of bulk ship’s hull
                    blocks in details through process of encoder data of the loader, FPFH feature matching and ICP algorithm. 
                  In the identification algorithm, we project real-time point clouds of the operation zone to spherical coordinate and transforms 
                    the 3D point clouds to 2D images for fast and reliable identification of operation target. 
                  Our method detects and complements four edges of the operation target through process of the 2D images and estimates both posture
                    and size of operation target in the bulk terminal based on the complemented edges. 
                  Experimental trials show that our algorithm allows us to achieve the reconstruction of hull blocks and real-time 
                    identification of operation target with high accuracy and reliability.          
                </div>
                <!-- <p>Status: Under Review</p> -->
                <p>Authors: <b>Yang Miao</b>, Changan Li, Zhan Li, Yipeng Yang, Xinghu Yu</p>
                <p style="font-size: 16px;"> <i><b>Measurement and Control, 2021</b> </i></p>
                <a href="https://journals.sagepub.com/doi/10.1177/0020294021992804">[pdf]</a>
            </div>
          </div>

      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
